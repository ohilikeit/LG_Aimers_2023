{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9NWbT0ZujlP8",
        "yHnaCEjCjxD_",
        "1A_5bF8soNF9"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1tumAFqTiXkwPiW_UXXFDm49UtAEKUEqf",
      "authorship_tag": "ABX9TyMvsH4cBS4F9jsQJ8AlWlpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohilikeit/LG_Aimers_2023/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna\n",
        "! pip install catboost"
      ],
      "metadata": {
        "id": "XDJ9aluJhK3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "oVJwOob_UgF2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "%matplotlib inline \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "from optuna import Trial, visualization\n",
        "from optuna.samplers import TPESampler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "import random\n",
        "import os\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "seed_everything(42)\n",
        "os.chdir('/content/drive/MyDrive/ML_projects/LG_Aimers')\n",
        "train = pd.read_csv('./train.csv').fillna(0)\n",
        "test = pd.read_csv('./test.csv').fillna(0)\n",
        "sample_submission = pd.read_csv('./sample_submission.csv')\n",
        "\n",
        "qual_col = ['LINE', 'PRODUCT_CODE']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train[i])\n",
        "    train[i] = le.transform(train[i])\n",
        "    \n",
        "    for label in np.unique(test[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test[i] = le.transform(test[i])\n",
        "\n",
        "X = train.drop(['PRODUCT_ID', 'Y_Class', 'Y_Quality', 'TIMESTAMP'], axis=1)\n",
        "y = train['Y_Class']\n",
        "test = test.drop(['PRODUCT_ID', 'TIMESTAMP'], axis=1)\n",
        "\n",
        "# scaling\n",
        "scaler = RobustScaler()\n",
        "X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
        "test = pd.DataFrame(scaler.transform(test), columns = test.columns)\n",
        "\n",
        "def preprocess(df):\n",
        "    # 아예 비어있는 컬럼 제거\n",
        "    for i in df.columns:\n",
        "        if list(df[i].value_counts()) == []:\n",
        "            df.drop(i, axis = 1, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "X = preprocess(X)\n",
        "test = preprocess(test)\n",
        "train = pd.concat([X, y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "ex = X.iloc[:, :2]\n",
        "only_X = X.iloc[:, 2:]\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(only_X)\n",
        "X_pca = pca.transform(only_X)\n",
        "print(X_pca.shape)\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "df = pd.DataFrame(X_pca, columns = ['comp1', 'comp2'])\n",
        "train = pd.concat([df, ex, y], axis=1)"
      ],
      "metadata": {
        "id": "wU3MqBX0yjfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "ex = X.iloc[:, :2]\n",
        "only_X = X.iloc[:, 2:]\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(only_X,y)\n",
        "X_lda = lda.transform(only_X)\n",
        "\n",
        "\n",
        "ex_2 = test.iloc[:, :2]\n",
        "only_X_test = test.iloc[:, 2:]\n",
        "test_lda = lda.transform(only_X_test)\n",
        "test = pd.DataFrame(test_lda, columns = ['comp1', 'comp2'])\n",
        "test = pd.concat([test, ex_2], axis=1)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(X_lda, columns = ['comp1', 'comp2'])\n",
        "train = pd.concat([df, ex, y], axis=1)\n",
        "X = train.drop('Y_Class', axis=1)\n",
        "y = train['Y_Class']"
      ],
      "metadata": {
        "id": "ZDxf81Lg0bTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습함수 정의 \n",
        "def objective(trial: Trial, train):\n",
        "    param = {\n",
        "        'verbose' : 0,\n",
        "        'random_state': 7,\n",
        "        'n_iter_no_change' : 50,\n",
        "        'validation_fraction' : 0.1,\n",
        "        'learning_rate' : 0.1,\n",
        "        'n_estimators' : trial.suggest_int('n_estimators', 100, 2000, step=20),\n",
        "        'min_impurity_decrease' : trial.suggest_float('min_impurity_decrease', 0, 0.2),\n",
        "        'max_depth' : trial.suggest_int('max_depth', 2, 20),\n",
        "        'max_features' : trial.suggest_categorical('max_features', ['auto', 'log2']),\n",
        "        'subsample' : trial.suggest_float('subsample', 0.7, 1.0, step=0.1)\n",
        "    }\n",
        "    cv_accuracy = []\n",
        "    cv = StratifiedKFold(n_splits = 5)\n",
        "    n_iter = 0\n",
        "\n",
        "    for t,v in cv.split(train, train['Y_Class']):\n",
        "        train_cv = train.iloc[t]\n",
        "        val_cv = train.iloc[v]\n",
        "\n",
        "        X = train_cv.drop('Y_Class', axis=1)\n",
        "        y = train_cv['Y_Class']\n",
        "\n",
        "        val_X = val_cv.drop('Y_Class', axis=1)\n",
        "        val_y = val_cv['Y_Class']\n",
        "\n",
        "        model = GradientBoostingClassifier(**param)\n",
        "\n",
        "        model.fit(X, y)\n",
        "        score = f1_score(val_y, model.predict(val_X), average = 'macro')\n",
        "\n",
        "        cv_accuracy.append(score)\n",
        "        n_iter += 1\n",
        "\n",
        "    return np.mean(cv_accuracy)\n",
        "\n",
        "    # model = GradientBoostingClassifier(**param)\n",
        "    # model.fit(X_train, y_train)\n",
        "    # score = f1_score(y_val, model.predict(X_val), average='macro')\n",
        "\n",
        "    # return score\n",
        "\n",
        "# create study\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),\n",
        "    pruner=optuna.pruners.HyperbandPruner(),\n",
        "    study_name='GB-Hyperparameter-Tuning'\n",
        ")\n",
        "\n",
        "# 학습 \n",
        "study.optimize(lambda trial: objective(trial,train), \n",
        "               n_trials=50)\n",
        "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ],
      "metadata": {
        "id": "zitAYWh51YrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC(kernel = 'poly)로 stacking이나 활용하자 / split은 0.2 기준 0 나왔음. \n",
        "# 할때 범주형 변수 labelencoding 해놨으니 얘네는 빼고 돌려보고 더 좋은 걸로 하자 \n",
        "\n",
        "# y_class 나뉘는걸 보면 굉장히 민감하게 나뉘는거같은데 pca와 같은 차원축소 기법이 과연 옳을 것인가는 의문\n",
        "# 이건 해보고 성능 높아지면 쓰고 아니면 안쓰기 \n",
        "\n",
        "# 공정 데이터이고 각 컬럼이 비식별화되어 함부로 판단하기 힘듦. \n",
        "# 결측치로 비어있는 부분은 아마 line이나 product code가 다름에 따라 해당 공정이 없는 경우일 것으로 예상됨.\n",
        "# 이상치 처리는 건들지 말고 결측치는 0으로 두어 모델에 반영되지 않게 하자. \n",
        "\n",
        "# line, product 4-1 이랑 5-1 조합 train에선 3 3 이고 test에선 3 1 인데 이거 빼고 아니고 성능 차이 확인 "
      ],
      "metadata": {
        "id": "yU5N35ZrkBca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GB_model = GradientBoostingClassifier(**study.best_trial.params, random_state = 42, n_iter_no_change = 50, validation_fraction = 0.1, learning_rate = 0.05)\n",
        "GB_model.fit(X, y)\n",
        "GB_pred = GB_model.predict(test)\n",
        "print(pd.DataFrame(GB_pred).value_counts())\n",
        "\n",
        "sample_submission['Y_Class'] = GB_pred\n",
        "sample_submission.to_csv('./submit.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFeRkwq13AND",
        "outputId": "31a1af29-7362-4b08-c8c3-cbe11375fe9c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    261\n",
            "0     30\n",
            "2     19\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. XGB"
      ],
      "metadata": {
        "id": "9NWbT0ZujlP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습함수 정의 \n",
        "def objective(trial: Trial, train):\n",
        "    param = {\n",
        "        'criterion' : 'absolute_error',\n",
        "        'tree_method' : 'exact',\n",
        "        'seed': 42, \n",
        "        'learning_rate' : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'n_estimators':trial.suggest_int('n_estimators', 100, 1000),        \n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 9),\n",
        "        'max_depth' : trial.suggest_int('max_depth', 3, 30),\n",
        "        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
        "        'min_split_loss' : trial.suggest_loguniform('min_split_loss', 1e-3, 10.0),\n",
        "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
        "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
        "    }\n",
        "    cv_accuracy = []\n",
        "    cv = StratifiedKFold(n_splits = 5)\n",
        "    n_iter = 0\n",
        "\n",
        "    for t,v in cv.split(train, train['Y_Class']):\n",
        "        train_cv = train.iloc[t]\n",
        "        val_cv = train.iloc[v]\n",
        "\n",
        "        X = train_cv.drop('Y_Class', axis=1)\n",
        "        y = train_cv['Y_Class']\n",
        "\n",
        "        val_X = val_cv.drop('Y_Class', axis=1)\n",
        "        val_y = val_cv['Y_Class']\n",
        "\n",
        "        model = XGBClassifier(**param)\n",
        "\n",
        "        model.fit(X, y)\n",
        "        score = f1_score(val_y, model.predict(val_X), average = 'macro')\n",
        "\n",
        "        cv_accuracy.append(score)\n",
        "        n_iter += 1\n",
        "\n",
        "    return np.mean(cv_accuracy)\n",
        "\n",
        "\n",
        "# create study\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),\n",
        "    pruner=optuna.pruners.HyperbandPruner(),\n",
        "    study_name='XGBClassifier-Hyperparameter-Tuning'\n",
        ")\n",
        "\n",
        "# 학습 \n",
        "study.optimize(lambda trial: objective(trial,train), \n",
        "               n_trials=50)\n",
        "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ],
      "metadata": {
        "id": "OI4oV31NhI4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. LGBM"
      ],
      "metadata": {
        "id": "yHnaCEjCjxD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습함수 정의 \n",
        "def objective(trial: Trial, train):\n",
        "\n",
        "    param = {\n",
        "        'verbose' : -1,\n",
        "        'random_state': 7, \n",
        "        'n_jobs': -1,\n",
        "        # 'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
        "        # 'max_depth' : trial.suggest_int('max_depth', 2, 30),\n",
        "        # 'n_estimators' : trial.suggest_int('n_estimators', 500, 3000),\n",
        "        # 'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.3), \n",
        "        'learning_rate' : 0.1,\n",
        "        # 'num_leaves' : trial.suggest_int('num_leaves', 5, 30),\n",
        "        # 'min_child_samples' : trial.suggest_int('min_child_samples', 2, 10),\n",
        "        # 'reg_alpha' : trial.suggest_float('reg_alpha', 1, 1.3),\n",
        "        # 'reg_lambda' : trial.suggest_float('reg_lambda', 1, 1.3)\n",
        "    }\n",
        "    cv_accuracy = []\n",
        "    cv = StratifiedKFold(n_splits = 5)\n",
        "    n_iter = 0\n",
        "\n",
        "    for t,v in cv.split(train, train['Y_Class']):\n",
        "        train_cv = train.iloc[t]\n",
        "        val_cv = train.iloc[v]\n",
        "\n",
        "        X = train_cv.drop('Y_Class', axis=1)\n",
        "        y = train_cv['Y_Class']\n",
        "\n",
        "        val_X = val_cv.drop('Y_Class', axis=1)\n",
        "        val_y = val_cv['Y_Class']\n",
        "\n",
        "        model = LGBMClassifier(**param)\n",
        "\n",
        "        model.fit(X, y)\n",
        "        score = f1_score(val_y, model.predict(val_X), average = 'macro')\n",
        "        print(score)\n",
        "\n",
        "        cv_accuracy.append(score)\n",
        "        n_iter += 1\n",
        "\n",
        "    return np.mean(cv_accuracy)\n",
        "\n",
        "# create study\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),\n",
        "    pruner=optuna.pruners.HyperbandPruner(),\n",
        "    study_name='LGBM-Hyperparameter-Tuning'\n",
        ")\n",
        "\n",
        "# 학습 \n",
        "study.optimize(lambda trial: objective(trial,train),\n",
        "               n_trials=100)\n",
        "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ],
      "metadata": {
        "id": "nuVz1quQjj6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, stratify = y, random_state = 7)\n",
        "\n",
        "RF = RandomForestClassifier(random_state=7).fit(X_train, y_train)\n",
        "RF_pred = RF.predict(X_val)\n",
        "score = f1_score(y_val, RF_pred, average='macro')\n",
        "print(score)"
      ],
      "metadata": {
        "id": "K1_RYnputf9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LGBMClassifier(random_state=7).fit(X_train, y_train)\n",
        "pred = model.predict(X_val)\n",
        "score = f1_score(y_val, pred, average='macro')\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MBs-n3VuOFd",
        "outputId": "1fa615fa-eabe-45b9-ffb3-4cb8a14aab50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5929319467250501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(random_state=7).fit(X_train, y_train)\n",
        "pred = model.predict(X_val)\n",
        "score = f1_score(y_val, pred, average='macro')\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQHkKEWmuh6q",
        "outputId": "45afc590-b92b-4c26-8942-461ff91d0cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5925925925925926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ExtraTreesClassifier(random_state=7).fit(X_train, y_train)\n",
        "pred = model.predict(X_val)\n",
        "score = f1_score(y_val, pred, average='macro')\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2funq1bCukIq",
        "outputId": "74861c81-d849-4a0f-e38e-d4e81e983d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6415491578535056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GradientBoostingClassifier(random_state=7).fit(X_train, y_train)\n",
        "pred = model.predict(X_val)\n",
        "score = f1_score(y_val, pred, average='macro')\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOvYFnmhy2Ak",
        "outputId": "644f7b5f-8596-414d-fd0e-0612eed1b03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForestClassifier(random_state=43).fit(X, y)\n",
        "RF_pred = RF.predict(test)\n",
        "\n",
        "XGB = XGBClassifier(random_state=43).fit(X, y)\n",
        "XGB_pred = XGB.predict(test)\n",
        "\n",
        "LGBM = LGBMClassifier(random_state=43).fit(X, y)\n",
        "LGBM_pred = LGBM.predict(test)\n",
        "\n",
        "ET = ExtraTreesClassifier(random_state=43).fit(X, y)\n",
        "ET_pred = ET.predict(test)\n",
        "\n",
        "GB = GradientBoostingClassifier(random_state=43).fit(X, y)\n",
        "GB_pred = GB.predict(test)\n",
        "\n",
        "# 2랑 5가 멀쩡 "
      ],
      "metadata": {
        "id": "vFe8-nPPyeb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5개 다 쓰는 hard voting\n",
        "# https://wikidocs.net/42408\n",
        "\n",
        "def mode(list):\n",
        "    count = 0\n",
        "    mode = 0;\n",
        "    for x in list: \n",
        "        if list.count(x) > count:\n",
        "            count = list.count(x)\n",
        "            mode = x\n",
        "\n",
        "    return mode\n",
        "final_pred = []\n",
        "for i in range(0, len(RF_pred)):\n",
        "    final_pred.append(mode([RF_pred[i], XGB_pred[i], LGBM_pred[i], ET_pred[i], GB_pred[i]]))\n",
        "sample_submission['Y_Class'] = final_pred\n",
        "sample_submission['Y_Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHtTCwlS10ti",
        "outputId": "bfb65b75-0b5b-4fbd-c276-9d674aa26a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    268\n",
              "0     40\n",
              "2      2\n",
              "Name: Y_Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mode(list):\n",
        "    count = 0\n",
        "    mode = 0;\n",
        "    for x in list: \n",
        "        if list.count(x) > count:\n",
        "            count = list.count(x)\n",
        "            mode = x\n",
        "    return mode\n",
        "final_pred = []\n",
        "for i in range(0, len(RF_pred)):\n",
        "    final_pred.append(mode([GB_pred[i]]))\n",
        "sample_submission['Y_Class'] = final_pred\n",
        "sample_submission['Y_Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl8fQo5j2dhv",
        "outputId": "564ff0ca-4acb-4c7f-9f3c-fae24816f1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    242\n",
              "0     39\n",
              "2     29\n",
              "Name: Y_Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.to_csv('./submit.csv', index=False)"
      ],
      "metadata": {
        "id": "B22ISQ7H2jj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ET"
      ],
      "metadata": {
        "id": "1A_5bF8soNF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습함수 정의 \n",
        "def objective(trial: Trial, train):\n",
        "    param = {\n",
        "        'verbose' : 1,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'max_depth' : trial.suggest_int('max_depth', 5, 50),\n",
        "        'ccp_alpha' : trial.suggest_float('ccp_alpha', 0, 0.3),\n",
        "        'n_estimators':trial.suggest_int('n_estimators', 500, 3000),\n",
        "        'min_impurity_decrease' : trial.suggest_float('min_impurity_decrease', 0, 0.5)\n",
        "    }\n",
        "    cv_accuracy = []\n",
        "    cv = StratifiedKFold(n_splits = 5)\n",
        "    n_iter = 0\n",
        "\n",
        "    for t,v in cv.split(train, train['Y_Class']):\n",
        "        train_cv = train.iloc[t]\n",
        "        val_cv = train.iloc[v]\n",
        "\n",
        "        X = train_cv.drop('Y_Class', axis=1)\n",
        "        y = train_cv['Y_Class']\n",
        "\n",
        "        val_X = val_cv.drop('Y_Class', axis=1)\n",
        "        val_y = val_cv['Y_Class']\n",
        "\n",
        "        model = ExtraTreesClassifier(**param)\n",
        "\n",
        "        model.fit(X, y)\n",
        "        score = f1_score(val_y, model.predict(val_X), average = 'macro')\n",
        "\n",
        "        cv_accuracy.append(score)\n",
        "        n_iter += 1\n",
        "\n",
        "    return np.mean(cv_accuracy)\n",
        "\n",
        "\n",
        "# create study\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),\n",
        "    pruner=optuna.pruners.HyperbandPruner(),\n",
        "    study_name='ExtraTrees-Hyperparameter-Tuning'\n",
        ")\n",
        "\n",
        "# 학습 \n",
        "study.optimize(lambda trial: objective(trial,train), \n",
        "               n_trials=50)\n",
        "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ],
      "metadata": {
        "id": "y7YZq7Bbkc_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. GB"
      ],
      "metadata": {
        "id": "Os-HbRdq7gv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 7)\n",
        "\n",
        "# 학습함수 정의 \n",
        "def objective(trial: Trial, train):\n",
        "    param = {\n",
        "        'verbose' : 0,\n",
        "        'random_state': 7,\n",
        "        'n_iter_no_change' : 50,\n",
        "        'validation_fraction' : 0.1,\n",
        "        'learning_rate' : 0.1,\n",
        "        'n_estimators' : trial.suggest_int('n_estimators', 100, 2000, step=20),\n",
        "        'min_impurity_decrease' : trial.suggest_float('min_impurity_decrease', 0, 0.2),\n",
        "        'max_depth' : trial.suggest_int('max_depth', 2, 20),\n",
        "        'max_features' : trial.suggest_categorical('max_features', ['auto', 'log2']),\n",
        "        'subsample' : trial.suggest_float('subsample', 0.7, 1.0, step=0.1)\n",
        "    }\n",
        "    cv_accuracy = []\n",
        "    cv = KFold(n_splits = 5)\n",
        "    n_iter = 0\n",
        "\n",
        "    for t,v in cv.split(train, train['Y_Class']):\n",
        "        train_cv = train.iloc[t]\n",
        "        val_cv = train.iloc[v]\n",
        "\n",
        "        X = train_cv.drop('Y_Class', axis=1)\n",
        "        y = train_cv['Y_Class']\n",
        "\n",
        "        val_X = val_cv.drop('Y_Class', axis=1)\n",
        "        val_y = val_cv['Y_Class']\n",
        "\n",
        "        model = GradientBoostingClassifier(**param)\n",
        "\n",
        "        model.fit(X, y)\n",
        "        score = f1_score(val_y, model.predict(val_X), average = 'macro')\n",
        "\n",
        "        cv_accuracy.append(score)\n",
        "        n_iter += 1\n",
        "\n",
        "    return np.mean(cv_accuracy)\n",
        "\n",
        "    # model = GradientBoostingClassifier(**param)\n",
        "    # model.fit(X_train, y_train)\n",
        "    # score = f1_score(y_val, model.predict(X_val), average='macro')\n",
        "\n",
        "    # return score\n",
        "\n",
        "# create study\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(seed=42),\n",
        "    pruner=optuna.pruners.HyperbandPruner(),\n",
        "    study_name='GB-Hyperparameter-Tuning'\n",
        ")\n",
        "\n",
        "# 학습 \n",
        "study.optimize(lambda trial: objective(trial,train), \n",
        "               n_trials=50)\n",
        "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ],
      "metadata": {
        "id": "oay2Gzkp7ht2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GB_model = GradientBoostingClassifier(random_state = 42, n_iter_no_change = 50, validation_fraction = 0.1, n_estimators = 1900, \n",
        "                                      learning_rate = 0.05, min_impurity_decrease = 0.07031308605404483, max_depth = 2)\n",
        "GB_model.fit(X, y)\n",
        "GB_pred = GB_model.predict(test)\n",
        "pd.DataFrame(GB_pred).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofq2_xYiB3w5",
        "outputId": "825b20e7-7449-4cfe-9d85-0d1cd59406be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    242\n",
              "2     39\n",
              "0     29\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GB_model = GradientBoostingClassifier(**study.best_trial.params, random_state = 42, n_iter_no_change = 50, validation_fraction = 0.1, learning_rate = 0.05)\n",
        "GB_model.fit(X, y)\n",
        "GB_pred = GB_model.predict(test)\n",
        "print(pd.DataFrame(GB_pred).value_counts())\n",
        "\n",
        "sample_submission['Y_Class'] = GB_pred\n",
        "sample_submission.to_csv('./submit.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxv63HuIY2lh",
        "outputId": "e3296ad1-4054-4f70-8eb4-f068d89ce95d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    241\n",
            "2     39\n",
            "0     30\n",
            "dtype: int64\n"
          ]
        }
      ]
    }
  ]
}